{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**Step 1:** We install PySpark which is the Python API for Apache Spark "
      ],
      "metadata": {
        "id": "qtJ0ASyYfOFt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyspark    # Installing pyspark Using pip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PIGJPGCasGKR",
        "outputId": "b5b8c92d-9eeb-4e32-b8b7-8f89f3f1e4e1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.7/dist-packages (3.3.0)\n",
            "Requirement already satisfied: py4j==0.10.9.5 in /usr/local/lib/python3.7/dist-packages (from pyspark) (0.10.9.5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 2:**\n",
        "Since PySpark uses Java, we need to have Java while using PySpark. We use Open JDK(or Open Java Development Kit) which is a open source and free implementation of Java Platform.\n",
        "\n",
        "Extra info for understanding: Java Development Kit (in short JDK) is Kit which provides the environment to Develop and execute(run ) the Java program. JDK contains Java Run Environment (JRE) and Java Virtual Machine(JVM)\n",
        "\n",
        "Here, '8' refers to the version and headless are software capable of working on a device without a graphical user interface. Such software receives inputs and provides output through other interfaces like network or serial port and is common on servers and embedded devices."
      ],
      "metadata": {
        "id": "gFoo9qY5ghDy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!apt install openjdk-8-jdk-headless -qq    "
      ],
      "metadata": {
        "id": "Dk6sEmopgC_s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 3:**  We need to set environment variables for the JDK\n",
        "\n",
        "**JAVA_HOME** is the name of an environment variable on the operating system that points to the installation directory of JDK (Java Development Kit) or JRE (Java Runtime Environment)\n",
        "\n",
        "Spark is written in the Scala programming language and requires the Java Virtual Machine to run. Therefore,we have to download Java inorder to proceed further"
      ],
      "metadata": {
        "id": "7TTcbFZ3gYYs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"JAVA_HOME\"]=\"/usr/lib/jvm/java-8-openjdk-amd64\" "
      ],
      "metadata": {
        "id": "TT5Ch7LOsPSF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 4:** **PyDrive** is installed, as it simplifies many common Google Drive API tasks. It has the following features:\n",
        "\n",
        "1. Simplifies OAuth2.0 into just few lines with flexible settings.\n",
        "\n",
        "2. Wraps Google Drive API into classes of each resource to make your program more object-oriented.\n",
        "\n",
        "3. Helps common operations else than API calls, such as content fetching and pagination control."
      ],
      "metadata": {
        "id": "6doW8LgThoVB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U -q PyDrive"
      ],
      "metadata": {
        "id": "nvW5DXCj2QEy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Google-auth** is the Google authentication library for Python. This library provides the ability to authenticate to Google APIs using various methods.\n",
        "\n",
        "**PyDrive** is a wrapper library of google-api-python-client that simplifies many common Google Drive API tasks\n",
        "\n",
        "**Credentials** are used to obtain an access token from Google's authorization servers "
      ],
      "metadata": {
        "id": "gE1OFcjz2Vvq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from pydrive.auth import GoogleAuth \n",
        "from pydrive.drive import GoogleDrive \n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)"
      ],
      "metadata": {
        "id": "9CUQs2jts4cq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 5:**Downloading the dataset"
      ],
      "metadata": {
        "id": "59qU7hnDk41h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fileDownloaded = drive.CreateFile({'id':'1BAPG9YpWbubt3lbvXBjgbufTM3Bvz9GL'}) #downloading the file \n",
        "fileDownloaded.GetContentFile('mini_spark_event_data.json')"
      ],
      "metadata": {
        "id": "e5sX6sXauD4B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 6:** Importing necessary libraries"
      ],
      "metadata": {
        "id": "IRjJkHnvk2P-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np #used for working with arrays\n",
        "\n",
        "import pandas as pd # provides high-performance data manipulation in Python\n",
        "\n",
        "from pyspark.sql import SparkSession  #SparkSession can be used create DataFrame , register DataFrame as tables, execute SQL over tables, cache tables, and read parquet files.\n",
        "\n",
        "from pyspark.sql.functions import udf #UDF is a User Defined Function that is used to create a reusable function in Spark\n",
        "\n",
        "from pyspark.sql import Window #PySpark Window function performs statistical operations such as rank, row number, etc. on a group, frame, or collection of rows and returns results for each row individually.\n",
        "\n",
        "from pyspark.sql import functions as F #importing module function inorder to define some functions\n",
        "\n",
        "from pyspark.sql.functions import sum as Fsum #In order to get the cumulative sum of column in pyspark we will be using sum function\n",
        "\n",
        "from pyspark.sql.types import ArrayType, BooleanType, LongType, FloatType, IntegerType #importing the required data types in order to perform certain operations\n",
        "\n",
        "from pyspark.sql.functions import lit, udf, struct, countDistinct, collect_list, avg, count, col\n",
        "# PySpark lit() function is used to add constant or literal value as a new column to the DataFrame\n",
        "# PySpark UDF is a User Defined Function that is used to create a reusable function in Spark\n",
        "# PySpark StructType & StructField classes are used to programmatically specify the schema to the DataFrame and create complex columns like nested struct, array, and map columns.\n",
        "# PySpark count distinct is a function used in PySpark that are basically used to count the distinct number of element in a PySpark Data frame\n",
        "# Collectlist is used to collect and return a list of non-unique elements\n",
        "# avg() in PySpark is used to return the average value from a particular column in the DataFrame\n",
        "# count() in PySpark is used to return the number of rows from a particular column in the DataFrame\n",
        "# col() in pyspark returns a Column based on the given column name.\n",
        "\n",
        "from pyspark.ml.feature import VectorAssembler, Normalizer, StandardScaler\n",
        "# VectorAssembler is a transformer that combines a given list of columns into a single vector column.\n",
        "# Normalizer is a Transformer which transforms a dataset of Vector rows, normalizing each Vector to have unit norm\n",
        "# The StandardScaler standardizes a set of features to have zero mean and a standard deviation of 1\n",
        "\n",
        "from pyspark.ml.classification import LogisticRegression, RandomForestClassifier, GBTClassifier\n",
        "# PySpark MLlib API provides a Classifier class to classify data using required method\n",
        "\n",
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator, BinaryClassificationEvaluator #used to evaluate the classifiers\n",
        "\n",
        "from pyspark.ml import Pipeline #A Pipeline consists of a sequence of stages, each of which is either an Estimator or a Transformer\n",
        "\n",
        "import matplotlib.pyplot as plt # collection of functions that make matplotlib work like MATLAB\n",
        "\n",
        "from sklearn.metrics import roc_curve #The roc_curve function calculates all FPR and TPR coordinates\n",
        "\n",
        "from sklearn.metrics import precision_recall_curve #Precision-Recall curves summarize the trade-off between the true positive rate and the positive predictive value for a predictive model using different probability thresholds\n",
        "\n",
        "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
        "# CrossValidator begins by splitting the dataset into a set of folds which are used as separate training and test datasets\n",
        "# ParamGridBuilder builds and returns all combinations of parameters specified by the param grid"
      ],
      "metadata": {
        "id": "pfbb9TaltWzQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 7:** Creating Spark Session"
      ],
      "metadata": {
        "id": "JFWUNJYapAz3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#creating  SPARK SESSION\n",
        "spark = SparkSession \\\n",
        ".builder \\\n",
        ".appname(\"Anisha_J_2211630_assignment4\")\\\n",
        ".getOrCreate() #getOrCreate is used to get the value of a parameter in the user-supplied parameter map or its default value."
      ],
      "metadata": {
        "id": "-g135jSUuA2W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 8**: Loading the dataset"
      ],
      "metadata": {
        "id": "cMl1wBSgpJBc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Loading the Data Set\n",
        "df = \"mini_spark_event_data.json\"\n",
        "df = spark.read.json(df)"
      ],
      "metadata": {
        "id": "VV1HLwV_uzmy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 9:** Cleaning the Dataset\n",
        "\n",
        "Dropping some irrelevant columns \n",
        "\n",
        "Droppping some potential NA values\n",
        "\n",
        "Filtering out the invalid Ids"
      ],
      "metadata": {
        "id": "dQSZNTYipOE2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cleaning the Dataset\n",
        "df = df.drop(*['artist','song','firstName', 'lastName', 'id_copy'])             \n",
        "df = df.dropna(how = 'any', subset = ['userId', 'sessionId'])                   \n",
        "df = df.filter(df.userId!='').orderBy([\"userId\", \"ts\"], ascending=[True, True]) \n",
        "df = df.withColumn(\"userId\", df[\"userId\"].cast(IntegerType()))"
      ],
      "metadata": {
        "id": "mUb-zyyPuzpX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 10:** Defining Churn"
      ],
      "metadata": {
        "id": "y1sQep4hsA5w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "“Churn” label is generated from the dataset by identifying users who confirm their subscription cancellation. Once churned users are identified, we can view how it behaves with other features in the dataset. We will be exploring the data to see trends and features that may influence the churn rate."
      ],
      "metadata": {
        "id": "1adMK9BGsHM1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining the Churn\n",
        "# Create a churn label column for the dataset. It returns 1 if Cancellation Confirmation events happens\n",
        "cancelation = udf(lambda x: 1 if x == \"Cancellation Confirmation\" else 0, \n",
        "                  IntegerType())  \n",
        "# Applying to the dataframe\n",
        "df = df.withColumn(\"churn\", cancelation(\"page\"))\n",
        "# Defining window bounds\n",
        "window = Window.partitionBy(\"userId\").rangeBetween(Window.unboundedPreceding, \n",
        "                                                   Window.unboundedFollowing)\n",
        "# Applying the window\n",
        "df = df.withColumn(\"churn\", Fsum(\"churn\").over(window))"
      ],
      "metadata": {
        "id": "uZiwvIVBuz1p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 11:** Distribution of Users by Churn Type\n",
        "\n",
        "Some new columns were made in this step to make the data exploration easier,the Feature engineering step comes later \n",
        "\n",
        "Making the level_shift Column\n",
        "\n",
        "This column tells us how many times did the customer switched from paid to free service"
      ],
      "metadata": {
        "id": "frZDKUXDs2RJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Distribution of users by Churn Type\n",
        "window1 = Window.partitionBy().orderBy([\"userId\", \"ts\"])\n",
        "df = df.withColumn(\"level_shift\", (df.level!=F.lag(df.level).over(window1)) | \n",
        "                   (df.userId!=F.lag(df.userId).over(window1)))\n",
        "df=df.fillna({'level_shift':0})\n",
        "df= df.withColumn(\"level_shift\", F.when(df[\"level_shift\"]==False, 0).otherwise(1))"
      ],
      "metadata": {
        "id": "D_bPeOxgwMqR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Making the last_ts column\n",
        "\n",
        "This column will help us to select only records that happened in thelast 2 weeks of customer activity\n",
        "\n",
        "The idea is that customer behavior should be different shortly before the churn happened"
      ],
      "metadata": {
        "id": "qjaGtELZ3SOW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df= df.withColumn(\"ts\", df.ts/1000)                        # trimming the last three zeros from the UNIX time (miliseconds)\n",
        "df= df.withColumn(\"registration\", df.registration/1000)"
      ],
      "metadata": {
        "id": "vl-w1VHtwMtB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "window2 = Window.partitionBy(\"userId\")\n",
        "df= df.withColumn(\"last_ts\", F.max('ts').over(window2))\n",
        "df=df.filter(df.last_ts - df.ts < 1300000) # approx no of sec in a 2 weeks"
      ],
      "metadata": {
        "id": "VR4KmkHNwMww"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Making the Columns: pages_per_session, diff_time\n",
        "pages_per_session is number of pages per session\n",
        "diff_time is a number of days since a specific page was visited"
      ],
      "metadata": {
        "id": "o6FzCQAp3bUm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "window3 = Window.partitionBy([\"userId\", \"sessionId\"])\n",
        "df= df.withColumn(\"pages_per_session\", F.max('ItemInSession').over(window3))\n",
        "df = df.withColumn(\"ts_time\",F.to_timestamp(df.ts))                    #  unix to datetime\n",
        "df = df.withColumn(\"last_ts_time\",F.to_timestamp(df.last_ts))          # unix to datetime"
      ],
      "metadata": {
        "id": "chVQrsr2yPgo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = df.withColumn(\"diff_time\",F.datediff(df.last_ts_time, df.ts_time))  # how many days ago was the page visited\n",
        "df=df.orderBy([\"userId\", \"ts\"], ascending=[True, True])                   \n",
        "df.createOrReplaceTempView('data');                                       # Create a Temp Table to be used for SQL queries"
      ],
      "metadata": {
        "id": "FS6XCNJlevoG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.limit(2).toPandas()\n",
        "# Here, I only want to display the first 2 rows.So,I limit it by 2 and then call toPandas() to return a pandas dataframe"
      ],
      "metadata": {
        "id": "t8cuvCdJz9jj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The column \"page\" seems to be most informative in the whole dataset\n",
        "\n",
        "It shows which pages of the service were visited by users, timestamp is also provided\n",
        "\n",
        "This column can be used to engineer useful features"
      ],
      "metadata": {
        "id": "QxTmEgce3ict"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.select('page','UserId').groupby('page').agg({'page':'count'}).select('page','count(page)').show()"
      ],
      "metadata": {
        "id": "r5_QhH6a0qfZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 12:** Printing the Schema"
      ],
      "metadata": {
        "id": "glmLmXpjtrvV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.printSchema():\n",
        "# used to print or display the schema of the DataFrame in the tree format along with column name and data type"
      ],
      "metadata": {
        "id": "G9RIC7PI0qhs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Investigation if there are differences between churned and non-churned users\n",
        "# label             - 0 if non_churned, 1 if churned\n",
        "# song_count        - avg number of songs played by churned/non_churned users\n",
        "# error             - avg number of errors occuring \n",
        "# friends           - avg number of \"friends\" on thhe application\n",
        "# playlist_count    - avg number of visits to the Playlist page\n",
        "# thumbs_up         - avg number of clicking the 'thumbs up'\n",
        "# thumbs_down       - avg number of clicking the 'thumbs down'\n",
        "# downgrade         - avg number of visits to the downgrade page\n",
        "# count_session_dist- avg number of sessions made\n",
        "# count_diff_time   - avg number of days (in the last two weeks) in which the user used the app\n",
        "# pages per session - avg numbers of pages (or any activity or changes) per session\n",
        "# duration          - avg number of days since the user joined (division with 86400 as a proxy of seconds within a day)\n",
        "# level_shift       - avg number of level changes (free, paid) per customer\n",
        "# usage_time        - avg total time spent using the app\n",
        "stats = spark.sql(\" WITH prep as( \\\n",
        "SELECT userId, \\\n",
        "max(churn)                                                          as label, \\\n",
        "count(case when page = 'NextSong' then userId else null end)        as song_count, \\\n",
        "count(case when page = 'Error' then userId else null end)           as error, \\\n",
        "count(case when page = 'Add Friend' then userId else null end)      as friends, \\\n",
        "count(case when page = 'Add to Playlist' then userId else null end) as playlist_count, \\\n",
        "count(case when page = 'Thumbs Up' then userId else null end)       as thumbs_up, \\\n",
        "count(case when page = 'Thumbs Down' then userId else null end)     as thumbs_down, \\\n",
        "count(case when page = 'Downgrade' then userId else null end)       as downgrade, \\\n",
        "count(distinct sessionId)                                           as count_session_dist, \\\n",
        "count(distinct diff_time)                                           as count_diff_time, \\\n",
        "avg(distinct pages_per_session)                                     as pages_per_session, \\\n",
        "(max(ts) - min(registration))/86400                                 as duration, \\\n",
        "sum(level_shift)                                                    as level_shift, \\\n",
        "sum(length)                                                         as usage_time \\\n",
        "FROM data \\\n",
        "GROUP BY userId) \\\n",
        "SELECT label, \\\n",
        "count(label)             as cnt, \\\n",
        "avg(song_count)          as song_count, \\\n",
        "avg(error)               as error, \\\n",
        "avg(friends)             as friends, \\\n",
        "avg(playlist_count)      as playlist_count, \\\n",
        "avg(thumbs_up)           as thumbs_up, \\\n",
        "avg(thumbs_down)         as thumbs_down, \\\n",
        "avg(downgrade)           as downgrade, \\\n",
        "avg(count_session_dist)  as count_session_dist, \\\n",
        "avg(count_diff_time)     as count_diff_time, \\\n",
        "avg(pages_per_session)   as pages_per_session, \\\n",
        "avg(duration)            as duration, \\\n",
        "avg (level_shift)        as level_shift, \\\n",
        "avg(usage_time)          as usage_time \\\n",
        "FROM prep \\\n",
        "GROUP BY label\")\n",
        "# We can see that for most dimensions/features there are differences between churned and non-churned users\n"
      ],
      "metadata": {
        "id": "BjLanhKy1Llz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stats.toPandas()\n",
        "#computes statistics for all numerical or string columns and then call toPandas() to return a pandas dataframe"
      ],
      "metadata": {
        "id": "TJb1toXF81Mj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 13:** Feature Engineering"
      ],
      "metadata": {
        "id": "YsTSG89kt_Nm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Feature Enigneering\n",
        "\n",
        "Based on the previous analysis, all 14 investigated features will be included.\n",
        "\n",
        "Here I am making a Temp Table which holds all the features.\n",
        "\n",
        "The temp table will be used as model input\n",
        "\n",
        "All data is aggregated per userId"
      ],
      "metadata": {
        "id": "l-EkLL7J32QX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "features = spark.sql(\"SELECT userId, \\\n",
        "max(churn)                                                          as label, \\\n",
        "count(case when page = 'NextSong' then userId else null end)        as song_count, \\\n",
        "count(case when page = 'Error' then userId else null end)           as error, \\\n",
        "count(case when page = 'Add Friend' then userId else null end)      as friends, \\\n",
        "count(case when page = 'Add to Playlist' then userId else null end) as playlist_count, \\\n",
        "count(case when page = 'Thumbs Up' then userId else null end)       as thumbs_up, \\\n",
        "count(case when page = 'Thumbs Down' then userId else null end)     as thumbs_down, \\\n",
        "count(case when page = 'Downgrade' then userId else null end)       as downgrade, \\\n",
        "count(distinct sessionId)                                           as count_session_dist, \\\n",
        "count(distinct diff_time)                                           as count_diff_time, \\\n",
        "round(avg(distinct pages_per_session),0)                            as pages_per_session, \\\n",
        "round((max(ts) - min(registration))/86400,0)                        as duration, \\\n",
        "round(sum(level_shift),0)                                           as level_shift, \\\n",
        "round(sum(length),0)                                                as usage_time \\\n",
        "FROM data \\\n",
        "GROUP BY userId\");\n",
        "features.createOrReplaceTempView('features');\n",
        "features=features.na.drop()"
      ],
      "metadata": {
        "id": "Z87He_DN1LoY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 14:** Splitting the dataset into test and train"
      ],
      "metadata": {
        "id": "MK7Gux9fuEfd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Train test Split\n",
        "training, test = feature.randomSplit([0.8,0.2])"
      ],
      "metadata": {
        "id": "JA92q85Z1Lrf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 15:** Assembling and Normalizing the data"
      ],
      "metadata": {
        "id": "LNennQtFuLqj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Make VectorAssembler - this is a Pypark specific step\n",
        "# All input features must be in one column before feeding into the model\n",
        "assembler = VectorAssembler(inputCols=[\"userId\",\"song_count\",\"error\",\"friends\",\"playlist_count\", \\\n",
        "                                       \"thumbs_up\",\"thumbs_down\",\"downgrade\", \"count_session_dist\",\\\n",
        "                                       \"count_diff_time\",\"pages_per_session\", \"duration\",\"level_shift\",\\\n",
        "                                       \"usage_time\"], \\\n",
        "                            outputCol=\"inputFeatures\")"
      ],
      "metadata": {
        "id": "YQWk6tYZ1Lvv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Normalizing the  Data\n",
        "scaler = Normalizer(inputCol = \"inputFeatures\", outputCol = \"features\")"
      ],
      "metadata": {
        "id": "Dd2kIKqR4JPr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 16:** Building  ML models"
      ],
      "metadata": {
        "id": "QjlCffcSuWkl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Fitting logistic regression,Gradient boosting and Random Forest classifier with default parameter"
      ],
      "metadata": {
        "id": "NOyBDd0_4EmV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lr = LogisticRegression\n",
        "gbt = GBTClassifier()\n",
        "rf = RandomForestClassifier()"
      ],
      "metadata": {
        "id": "sBZP5u2_4UBQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 17:** Building Pipelines"
      ],
      "metadata": {
        "id": "gpxvOAgcukCe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Building pipelines\n",
        "pipeline1 = Pipeline(stages = [assembler,scaler, lr])\n",
        "pipeline2 = Pipeline(stages = [assembler,scaler, gbt])\n",
        "pipeline3 = Pipeline(stages = [assembler,scaler, rf])"
      ],
      "metadata": {
        "id": "Gs4ODSpS5VDZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# metric chosen is f1 (we want to catch true positives (churn customers), but\n",
        "# we do not want to waste money on false positives (investing in retaining \n",
        "# non-churn customers, which are loyal anyway)\n",
        "# Note that Recall might also be justified to use here (if the cost offalsepositives is low)\n",
        "\n",
        "paramgrid =ParamGridBuilder()\\\n",
        ".addGrid(lr.regParam, [0.0, 0,1])\\\n",
        ".addGrid(lr.maxIter, [10])\\\n",
        ".build()"
      ],
      "metadata": {
        "id": "0l0U7FdR6Ili"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 18:** Evaluating and Cross Validating"
      ],
      "metadata": {
        "id": "eCFgkSz76TsI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "evaluator = MulticlassClassificationEvaluator(metricName = \"f1\")"
      ],
      "metadata": {
        "id": "vixx_7Kq6UX_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "crossval= CrossValidator(estimator=pipeline1,  \n",
        "                         estimatorParamMaps=paramgrid,\n",
        "                         evaluator = evaluator , \n",
        "                         numFolds=3\n",
        "                        )\n",
        "# K-fold cross validation performs model selection by splitting the dataset into a set of non-overlapping randomly partitioned folds which are used as separate training and test datasets.\n",
        "# e.g., with k=3 folds, K-fold cross validation will generate 3 (training, test) dataset pairs, each of which uses 2/3 of the data for training and 1/3 for testing. Each fold is used as the test set exactly once."
      ],
      "metadata": {
        "id": "SPRH0Uvv61Rv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cvModel1 = crosscval.fit(training) #fitting the model to training data"
      ],
      "metadata": {
        "id": "NvQFKqrD7MOI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "evaluator.evaluate(cvModel1.transform(test)) #evaluating the model"
      ],
      "metadata": {
        "id": "eI6AT81_7MQH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 19 :Gradient boosting Classifier**"
      ],
      "metadata": {
        "id": "6sP0kbo079Lx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Specify multiple parameters in the paramgrid, in case you have\n",
        "# enough processing power \n",
        "paramgrid1 =ParamGridBuilder()\\\n",
        ".build()\n",
        "\n",
        "evaluator = MulticlassClassificationEvaluator(metricName ='f1')"
      ],
      "metadata": {
        "id": "ehwzCt1d78bg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "crossval= CrossValidator(estimator=pipeline2,  \n",
        "                         estimatorParamMaps=paramgrid1,\n",
        "                         evaluator=evaluator, \n",
        "                         numFolds=3\n",
        "                        )"
      ],
      "metadata": {
        "id": "GixzX7mN9QOf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cvModel2 = crossval.fit(training) #fitting the model to training data"
      ],
      "metadata": {
        "id": "5Ql_8mhE9MIs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "evaluator.evaluate(cvModel2.transform(test)) #evaluating the model"
      ],
      "metadata": {
        "id": "453Y5S168D01"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 20 :Random Forest Classifier**"
      ],
      "metadata": {
        "id": "bsdfKF3p93pR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Specify multiple parameters in the paramgrid, in case you have enough processign power\n",
        "paramgrid2 = ParamGridBuilder().build()"
      ],
      "metadata": {
        "id": "cBI5TudH-ANg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "evaluator = MulticlassClassificationEvaluator(metricName ='f1')\n"
      ],
      "metadata": {
        "id": "GeQSPXSV-AQW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "crossval= CrossValidator(estimator=pipeline3,  \n",
        "                         estimatorParamMaps=paramgrid2,\n",
        "                         evaluator=evaluator, \n",
        "                         numFolds=3\n",
        "                        )"
      ],
      "metadata": {
        "id": "X2j5CrGa-hBM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cvModel3 = crossval.fit(training) #fitting the model to training data"
      ],
      "metadata": {
        "id": "mMAvVtgn-lAY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "evaluator.evaluate(cvModel3.transform(test)) #Evaluates the output with optional parameters."
      ],
      "metadata": {
        "id": "U4qsha-9-lxg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predictions = cvmodel2 #making predictions"
      ],
      "metadata": {
        "id": "uDD2IAY8-lz_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predictions = cvModel2.transform(test)\n",
        "predictiona.limit(2).toPandas()\n",
        "# Here, I only want to display the first 2 rows.So,I limit it by 2 and then call toPandas() to return a pandas dataframe"
      ],
      "metadata": {
        "id": "AnBItjAz_8Wf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Step 21: ROC curve**\n",
        "\n",
        "A ROC curve is constructed by plotting the true positive rate (TPR) against the false positive rate (FPR)."
      ],
      "metadata": {
        "id": "QsVbPUuN_94Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#defining a ROC curve\n",
        "def roc(ax, predictions, labels, title='ROC curve'):\n",
        "    pp = predictions.toPandas()['probability'].apply(lambda x:x[1]).values\n",
        "    tpr, fpr, _ = roc_curve(labels, pp)\n",
        "    ax.plot(tpr, fpr) #plotting true positive rate and false positive rate\n",
        "    ax.set_facecolor('xkcd:wheat')\n",
        "    ax.set_xlabel('False Positive Rate') #setting up X and Y labels\n",
        "    ax.set_ylabel('True Positive Rate')\n",
        "    ax.set_title(title) #setting the title\n",
        "    \n",
        "#plt.clf() # to be used for AWS EMR \n",
        "labels=predictions.toPandas()['label']\n",
        "fig = plt.figure()\n",
        "ax = fig.add_subplot(111) #adding subplots \n",
        "roc(ax, predictions,labels)"
      ],
      "metadata": {
        "id": "oBs6qsGA_8Pm"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}