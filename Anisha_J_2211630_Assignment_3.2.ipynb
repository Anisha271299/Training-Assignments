{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **K-Means Using SPARk**"
      ],
      "metadata": {
        "id": "XwAtDDzJo6Ta"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **1.Setting up Spark on Google Colab Environment**"
      ],
      "metadata": {
        "id": "JyCxnV7ZpDfo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Lets setup spark by running the following codes**"
      ],
      "metadata": {
        "id": "4_umNfHipO-_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Installing pyspark**"
      ],
      "metadata": {
        "id": "aCSNaVF6rdW1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y50R55qznMOZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9e4f8b1b-87b3-4825-9f25-55c9e540f3a6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pyspark\n",
            "  Downloading pyspark-3.3.0.tar.gz (281.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 281.3 MB 47 kB/s \n",
            "\u001b[?25hCollecting py4j==0.10.9.5\n",
            "  Downloading py4j-0.10.9.5-py2.py3-none-any.whl (199 kB)\n",
            "\u001b[K     |████████████████████████████████| 199 kB 57.7 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.3.0-py2.py3-none-any.whl size=281764026 sha256=8385a8856ab5f414aa671effa7384bb5661505a9fdb5e0115f6493387db56c08\n",
            "  Stored in directory: /root/.cache/pip/wheels/7a/8e/1b/f73a52650d2e5f337708d9f6a1750d451a7349a867f928b885\n",
            "Successfully built pyspark\n",
            "Installing collected packages: py4j, pyspark\n",
            "Successfully installed py4j-0.10.9.5 pyspark-3.3.0\n"
          ]
        }
      ],
      "source": [
        "!pip install pyspark"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Installing pydrive as it is a high-level Python wrapper for the Google Drive API. \n",
        "It allows us to easily upload, download, and delete files in our Google Drive from a Python script.**"
      ],
      "metadata": {
        "id": "5bz3QImSriBF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U -q PyDrive"
      ],
      "metadata": {
        "id": "MXZ4q9HhjQrC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Spark is written in the Scala programming language and requires the Java Virtual Machine to run. Therefore,we have to download Java inorder to proceed further.**"
      ],
      "metadata": {
        "id": "H7ueevtCrmf3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!apt install openjdk-8-jdk-headless -qq"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5ndNZ_nOjZld",
        "outputId": "20d6f758-2608-4ac3-a326-fdcec72450ce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-460\n",
            "Use 'apt autoremove' to remove it.\n",
            "The following additional packages will be installed:\n",
            "  openjdk-8-jre-headless\n",
            "Suggested packages:\n",
            "  openjdk-8-demo openjdk-8-source libnss-mdns fonts-dejavu-extra\n",
            "  fonts-ipafont-gothic fonts-ipafont-mincho fonts-wqy-microhei\n",
            "  fonts-wqy-zenhei fonts-indic\n",
            "The following NEW packages will be installed:\n",
            "  openjdk-8-jdk-headless openjdk-8-jre-headless\n",
            "0 upgraded, 2 newly installed, 0 to remove and 22 not upgraded.\n",
            "Need to get 36.6 MB of archives.\n",
            "After this operation, 143 MB of additional disk space will be used.\n",
            "Selecting previously unselected package openjdk-8-jre-headless:amd64.\n",
            "(Reading database ... 123941 files and directories currently installed.)\n",
            "Preparing to unpack .../openjdk-8-jre-headless_8u342-b07-0ubuntu1~18.04_amd64.deb ...\n",
            "Unpacking openjdk-8-jre-headless:amd64 (8u342-b07-0ubuntu1~18.04) ...\n",
            "Selecting previously unselected package openjdk-8-jdk-headless:amd64.\n",
            "Preparing to unpack .../openjdk-8-jdk-headless_8u342-b07-0ubuntu1~18.04_amd64.deb ...\n",
            "Unpacking openjdk-8-jdk-headless:amd64 (8u342-b07-0ubuntu1~18.04) ...\n",
            "Setting up openjdk-8-jre-headless:amd64 (8u342-b07-0ubuntu1~18.04) ...\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/orbd to provide /usr/bin/orbd (orbd) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/servertool to provide /usr/bin/servertool (servertool) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/tnameserv to provide /usr/bin/tnameserv (tnameserv) in auto mode\n",
            "Setting up openjdk-8-jdk-headless:amd64 (8u342-b07-0ubuntu1~18.04) ...\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/idlj to provide /usr/bin/idlj (idlj) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/wsimport to provide /usr/bin/wsimport (wsimport) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/jsadebugd to provide /usr/bin/jsadebugd (jsadebugd) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/native2ascii to provide /usr/bin/native2ascii (native2ascii) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/javah to provide /usr/bin/javah (javah) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/clhsdb to provide /usr/bin/clhsdb (clhsdb) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/xjc to provide /usr/bin/xjc (xjc) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/hsdb to provide /usr/bin/hsdb (hsdb) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/schemagen to provide /usr/bin/schemagen (schemagen) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/extcheck to provide /usr/bin/extcheck (extcheck) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/jhat to provide /usr/bin/jhat (jhat) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/wsgen to provide /usr/bin/wsgen (wsgen) in auto mode\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Now that we have installed all the necessary dependencies in Colab, it is time to set the environment path. The below code will set up the environment path for JAVA**"
      ],
      "metadata": {
        "id": "IGUcvoF2sDp1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\""
      ],
      "metadata": {
        "id": "6uE1GNU7jiST"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Now, lets import the necesary libraries which we usually use**"
      ],
      "metadata": {
        "id": "7T8Fy64DsmN5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as pyplot\n",
        "%matplotlib  inline"
      ],
      "metadata": {
        "id": "FwLeXIe6j3cV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Now we have to import pyspark and along with some required modules in order to proceed** "
      ],
      "metadata": {
        "id": "BTEjLcqktOBp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pyspark\n",
        "from pyspark.sql import *   #Spark module for structured data processing in Python\n",
        "from pyspark.sql.types import *  #to create DataFrame with a specific type\n",
        "from pyspark.sql.functions import * #importing module function inorder to define some functions\n",
        "from pyspark import SparkContext, SparkConf"
      ],
      "metadata": {
        "id": "KXIC_Bq-tNmC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**We have imported SparkContext and SparkConf. Lets understand the usecare of these modules:**\n",
        "\n",
        "**SparkContext** represents the connection to a Spark cluster, and can be used to create RDD(resilient distributed datasets ) and broadcast variables on that cluster.\n",
        "Sparkcontext is the entry point for spark environment.\n",
        "\n",
        "**Sparkconf** is the class which gives us the various option to provide configuration parameters\n"
      ],
      "metadata": {
        "id": "a-ZUykF8xipP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Now, its time to initialize SparkContex and SparkConf**"
      ],
      "metadata": {
        "id": "K5Z99tthyBfI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#To create a SparkContext, first we need to build a SparkConf object that contains information about our application\n",
        "#let’s configure Spark ui first\n",
        "#creating the session\n",
        "conf = SparkConf().set(\"spark.ui.port\",\"4050\")"
      ],
      "metadata": {
        "id": "e7Yzba9cx4SK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Can easily check the current version and get the link of the web interface.\n",
        "In the Spark UI, I can monitor the progress of my job and debug the performance bottlenecks."
      ],
      "metadata": {
        "id": "nQVctthgmGao"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#creating the context\n",
        "sc = pyspark.SparkContext(conf=conf)\n",
        "spark = SparkSession.builder.getOrCreate() \n",
        "#getOrCreate is used to get the value of a parameter in the user-supplied parameter map or its default value."
      ],
      "metadata": {
        "id": "soGYoFc_lrj_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**After getting done with the configuration settings and initiating a SparkContext object,which Spark does by default. lets check it now so that we will get full details about**"
      ],
      "metadata": {
        "id": "ExEZpzdo07cl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "spark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 219
        },
        "id": "2Rp17RcR1DZD",
        "outputId": "8d3f67c5-2226-49c6-b29e-4b614e803187"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyspark.sql.session.SparkSession at 0x7fe386d09750>"
            ],
            "text/html": [
              "\n",
              "            <div>\n",
              "                <p><b>SparkSession - in-memory</b></p>\n",
              "                \n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://53355f0063e3:4050\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.3.0</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local[*]</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>pyspark-shell</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        \n",
              "            </div>\n",
              "        "
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "By running the below code on the Google colab hosted runtime, the cell below will create a ngrok tunnel which will allows us to still check the Spark UI."
      ],
      "metadata": {
        "id": "-JSgWuWwvUXn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n",
        "!unzip ngrok-stable-linux-amd64.zip\n",
        "get_ipython().system_raw('./ngrok http 4050 &')\n",
        "!curl -s http://localhost:4040/api/tunnels | python3 -c \\\n",
        "    \"import sys, json; print(json.load(sys.stdin)['tunnels'][0]['public_url'])\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i7c08tIH1uTs",
        "outputId": "27bdd5e5-ab77-440f-bb94-e1ea71222b93"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-10-20 05:21:39--  https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n",
            "Resolving bin.equinox.io (bin.equinox.io)... 54.237.133.81, 52.202.168.65, 18.205.222.128, ...\n",
            "Connecting to bin.equinox.io (bin.equinox.io)|54.237.133.81|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 13832437 (13M) [application/octet-stream]\n",
            "Saving to: ‘ngrok-stable-linux-amd64.zip’\n",
            "\n",
            "ngrok-stable-linux- 100%[===================>]  13.19M  19.6MB/s    in 0.7s    \n",
            "\n",
            "2022-10-20 05:21:40 (19.6 MB/s) - ‘ngrok-stable-linux-amd64.zip’ saved [13832437/13832437]\n",
            "\n",
            "Archive:  ngrok-stable-linux-amd64.zip\n",
            "  inflating: ngrok                   \n",
            "Traceback (most recent call last):\n",
            "  File \"<string>\", line 1, in <module>\n",
            "IndexError: list index out of range\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **2.Data Preprocessing**"
      ],
      "metadata": {
        "id": "p1CaA1zl1-HU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Now, lets import the required dataset and proceed with data preprocessing**"
      ],
      "metadata": {
        "id": "xHOo4ClP2DFx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_breast_cancer #importing breast cancer dataset and loading it\n",
        "breast_cancer = load_breast_cancer()"
      ],
      "metadata": {
        "id": "YqUMrnZK16sk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**For our convenience, given the dataset is small, we first construct a Pandas dataframe, tune the schema and then convert it into a Spark Dataframe**"
      ],
      "metadata": {
        "id": "I2v_c9QH2siw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pd_df = pd.DataFrame(breast_cancer.data, columns= breast_cancer.feature_names)\n",
        "df = spark.createDataFrame(pd_df)\n",
        "df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kZNsfDcO2mSC",
        "outputId": "090f2e68-2936-4388-825f-f91b9e94f4e0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DataFrame[mean radius: double, mean texture: double, mean perimeter: double, mean area: double, mean smoothness: double, mean compactness: double, mean concavity: double, mean concave points: double, mean symmetry: double, mean fractal dimension: double, radius error: double, texture error: double, perimeter error: double, area error: double, smoothness error: double, compactness error: double, concavity error: double, concave points error: double, symmetry error: double, fractal dimension error: double, worst radius: double, worst texture: double, worst perimeter: double, worst area: double, worst smoothness: double, worst compactness: double, worst concavity: double, worst concave points: double, worst symmetry: double, worst fractal dimension: double]"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def set_df_columns_nullable(spark, df, column_list, nullable=False):\n",
        "    for struct_field in df.schema:\n",
        "        if struct_field.name in column_list:\n",
        "            struct_field.nullable = nullable\n",
        "    df_mod = spark.createDataFrame(df.rdd, df.schema)\n",
        "    return df_mod\n",
        "df = set_df_columns_nullable(spark, df, df.columns)\n",
        "df = df.withColumn('features', array(df.columns))\n",
        "vectors = df.rdd.map(lambda row: Vectors.dense(row.features))\n",
        "\n",
        "df.printSchema()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bysB1SFK3lXH",
        "outputId": "7f3b823a-8813-4d0d-e5c3-f66c5bd2ba66"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- mean radius: double (nullable = false)\n",
            " |-- mean texture: double (nullable = false)\n",
            " |-- mean perimeter: double (nullable = false)\n",
            " |-- mean area: double (nullable = false)\n",
            " |-- mean smoothness: double (nullable = false)\n",
            " |-- mean compactness: double (nullable = false)\n",
            " |-- mean concavity: double (nullable = false)\n",
            " |-- mean concave points: double (nullable = false)\n",
            " |-- mean symmetry: double (nullable = false)\n",
            " |-- mean fractal dimension: double (nullable = false)\n",
            " |-- radius error: double (nullable = false)\n",
            " |-- texture error: double (nullable = false)\n",
            " |-- perimeter error: double (nullable = false)\n",
            " |-- area error: double (nullable = false)\n",
            " |-- smoothness error: double (nullable = false)\n",
            " |-- compactness error: double (nullable = false)\n",
            " |-- concavity error: double (nullable = false)\n",
            " |-- concave points error: double (nullable = false)\n",
            " |-- symmetry error: double (nullable = false)\n",
            " |-- fractal dimension error: double (nullable = false)\n",
            " |-- worst radius: double (nullable = false)\n",
            " |-- worst texture: double (nullable = false)\n",
            " |-- worst perimeter: double (nullable = false)\n",
            " |-- worst area: double (nullable = false)\n",
            " |-- worst smoothness: double (nullable = false)\n",
            " |-- worst compactness: double (nullable = false)\n",
            " |-- worst concavity: double (nullable = false)\n",
            " |-- worst concave points: double (nullable = false)\n",
            " |-- worst symmetry: double (nullable = false)\n",
            " |-- worst fractal dimension: double (nullable = false)\n",
            " |-- features: array (nullable = false)\n",
            " |    |-- element: double (containsNull = false)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "from the below cell, I am going build the two datastructures that we will be using throughout this Notebook namely,\n",
        "\n",
        "**features:** a dataframe of Dense vectors, containing all the original features in the dataset;\n",
        "\n",
        "**labels:** a series of binary labels indicating if the corresponding set of features belongs to a subject with breast cancer, or not."
      ],
      "metadata": {
        "id": "fvKTSyCevy6N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.linalg import Vectors\n",
        "features = spark.createDataFrame(vectors.map(Row), [\"features\"])\n",
        "labels = pd.Series(breast_cancer.target)"
      ],
      "metadata": {
        "id": "AscGqy3t34af"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **3.Building machine learning model**"
      ],
      "metadata": {
        "id": "cnx20C4mwHHv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now I am going to cluster the data with the K-means algorithm included in MLlib (Spark's Machine Learning library). Also, I am setting the k parameter to 2, because we have only two classes in the dataset. Then I am fitting the model, and the computing the Silhouette score) (i.e., a measure of quality of the obtained clustering).\n",
        "\n",
        "Here,I am using the MLlib implementation of the Silhouette score via ClusteringEvaluator"
      ],
      "metadata": {
        "id": "oHQZzfuMwPQo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#importing necessary libraries to build a KMeans model using pyspark\n",
        "from pyspark.ml.clustering import KMeans\n",
        "from pyspark.ml.evaluation import ClusteringEvaluator\n",
        "\n",
        "\n",
        "#Training a K-Means Model\n",
        "kmeans = KMeans().setK(2).setSeed(1)\n",
        "model = kmeans.fit(features)\n",
        "\n",
        "#Making pedictions\n",
        "predictions = model.transform(features)\n",
        "\n",
        "#Evaluating clustering by computing Silhouette  score\n",
        "evaluator = ClusteringEvaluator()\n",
        "\n",
        "silhouette = evaluator.evaluate(predictions)\n",
        "print(\"Silhouette with squared euclidean distance = \" + str(silhouette))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ug_9xs4iAEfF",
        "outputId": "3a3c88e0-aa8e-47aa-b51b-8a242104ab1c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Silhouette with squared euclidean distance = 0.8342904262826145\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here, Clustering Evaluator is used for Clustering results, which expects two input columns: prediction and features. The metric computes the Silhouette measure using the squared Euclidean distance.\n",
        "\n",
        "The Silhouette is a measure for the validation of the consistency within clusters. It ranges between 1 and -1, where a value close to 1 means that the points in a cluster are close to the other points in the same cluster and far from the points of the other clusters.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "CAHfHIQ0w89T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Silhouette Score using ClusteringEvaluator() measures how close each point in one cluster is to points in the neighboring clusters thus helping in figuring out clusters that are compact and well-spaced out."
      ],
      "metadata": {
        "id": "OUKpycYRIh4K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **4.Conclusion**"
      ],
      "metadata": {
        "id": "daS0GN5Dxlbx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, we build a K-Means Clustering model using 2 clusters by pyspark for breast cancer dataset."
      ],
      "metadata": {
        "id": "_domqN5gyAuU"
      }
    }
  ]
}